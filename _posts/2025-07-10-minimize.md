---
title: minimize
description: >-
  An introduction to the functions in /BatchOptim/minimize/
authors: [pengxin, xin]
date: 2025-07-10 22:26:00 +0800
categories: [Tutorials, BatchOptim, minimize]
tags: [BatchOptim, minimize]
---

## CG

(

`iter_scheme`: `Literal['PR+', 'PR', 'FR', 'SD', 'WYL']`

`E_threshold`: `<class 'float'> = 0.001`

`F_threshold`: `<class 'float'> = 0.05`

`maxiter`: `<class 'int'> = 100`

`linesearch`: `Literal['Backtrack', 'Wolfe', 'NWolfe', '2PT', '3PT', 'Golden', 'Newton', 'None'] = Backtrack`

`linesearch_maxiter`: `<class 'int'> = 10`

`linesearch_thres`: `<class 'float'> = 0.02`

`linesearch_factor`: `<class 'float'> = 0.6`

`steplength`: `<class 'float'> = 0.5`

`use_bb`: `<class 'bool'> = True`

`device`: `str | torch.device = cpu`

`verbose`: `<class 'int'> = 2`

)


    Conjugate Gradient Algo.
    Args:
        iter_scheme: Literal['PR+', 'PR', 'FR', 'SD', 'WYL'],
        E_threshold: float = 1e-3,
        F_threshold: float = 0.05,
        maxiter: int = 100,
        linesearch: Literal['Backtrack', 'Wolfe', 'NWolfe', '2PT', '3PT', 'Golden', 'Newton', 'None'] = 'Backtrack',
        linesearch_maxiter: int = 10,
        linesearch_thres: float = 0.02,
        linesearch_factor: float = 0.6,
        steplength: float = 0.5,
        use_bb: whether to use Barzilai-Borwein steplength (BB1 or long BB) as initial steplength instead of fixed one.
        device: str | th.device = 'cpu',
        verbose: int = 2
    

## initialize_algo_param

(

)


        Override this method to initialize attribute variables for self._update_direction.

        Returns: None
        

## run

(

`func`: `Any | torch.nn.modules.module.Module`

`X`: `<class 'torch.Tensor'>`

`grad_func`: `Any | torch.nn.modules.module.Module = None`

`func_args`: `Sequence = ()`

`func_kwargs`: `Optional[Dict] = None`

`grad_func_args`: `Sequence = ()`

`grad_func_kwargs`: `Optional[Dict] = None`

`is_grad_func_contain_y`: `<class 'bool'> = True`

`output_grad`: `<class 'bool'> = False`

`fixed_atom_tensor`: `Optional[torch.Tensor] = None`

`batch_indices`: `Union[NoneType, List[int], Tuple[int, ...], torch.Tensor] = None`

)


        Run the Optimization Algorithm.

        Parameters:
            func: the main function of instantiated torch.nn.Module class.
            X: Tensor[n_batch, n_atom, 3], the atom coordinates that input to func.
            grad_func: user-defined function that grad_func(X, ...) returns the func's gradient at X. if None, grad_func(X, ...) = th.autograd.grad(func(X, ...), X).
            func_args: optional, other input of func.
            func_kwargs: optional, other input of func.
            grad_func_args: optional, other input of grad_func.
            grad_func_kwargs: optional, other input of grad_func.
            is_grad_func_contain_y: bool, if True, grad_func contains output of func followed by X
                i.e., grad = grad_func(X, y, *grad_func_args, **grad_func_kwargs), else grad = grad_func(X, *grad_func_args, **grad_func_kwargs)
            output_grad: bool, whether output gradient of last step.
            fixed_atom_tensor: Optional[th.Tensor], the indices of X that fixed.
            batch_indices: Sequence | th.Tensor | np.ndarray | None, the split points for given X, Element_list & V_init, must be 1D integer array_like.
                the format of batch_indices is same as `split_size_or_sections` in torch.split:
                batch_indices = (n1, n2, ..., nN) will split X, Element_list & V_init into N parts, and ith parts has ni atoms. sum(n1, ..., nN) = X.shape[1]

        Return:
            min func: Tensor(n_batch, ), the minimum of func.
            argmin func: Tensor(X.shape), the X corresponds to min func.
            grad of argmin func: Tensor(X.shape), only output when `output_grad` == True. The gradient of X corresponding to minimum.
        

## QN

(

`iter_scheme`: `Literal['BFGS', 'Newton']`

`E_threshold`: `<class 'float'> = 0.001`

`F_threshold`: `<class 'float'> = 0.05`

`maxiter`: `<class 'int'> = 100`

`linesearch`: `Literal['Backtrack', 'Wolfe', 'NWolfe', '2PT', '3PT', 'Golden', 'Newton', 'None'] = Backtrack`

`linesearch_maxiter`: `<class 'int'> = 10`

`linesearch_thres`: `<class 'float'> = 0.02`

`linesearch_factor`: `<class 'float'> = 0.6`

`steplength`: `<class 'float'> = 0.5`

`use_bb`: `<class 'bool'> = True`

`external_Hessian`: `torch.Tensor | None = None`

`device`: `str | torch.device = cpu`

`verbose`: `<class 'int'> = 2`

)


    Quasi-Newton Algo.

    Args:
        iter_scheme: Literal['BFGS', 'Newton'],
        E_threshold: float = 1e-3,
        F_threshold: float = 0.05,
        maxiter: int = 100,
        linesearch: Literal['Backtrack', 'Wolfe', 'NWolfe', '2PT', '3PT', 'Golden', 'Newton', 'None'] = 'Backtrack',
        linesearch_maxiter: int = 10,
        linesearch_thres: float = 0.02,
        linesearch_factor: float = 0.6,
        steplength: float = 0.5,
        use_bb: whether to use Barzilai-Borwein steplength (BB1 or long BB) as initial steplength instead of fixed one.
        external_Hessian: manually input Hessian matrix as initial guess.
        device: str | th.device = 'cpu',
        verbose: int = 2
    

## initialize_algo_param

(

)


        Override this method to initialize attribute variables for self._update_direction.

        Returns: None
        

## run

(

`func`: `Any | torch.nn.modules.module.Module`

`X`: `<class 'torch.Tensor'>`

`grad_func`: `Any | torch.nn.modules.module.Module = None`

`func_args`: `Sequence = ()`

`func_kwargs`: `Optional[Dict] = None`

`grad_func_args`: `Sequence = ()`

`grad_func_kwargs`: `Optional[Dict] = None`

`is_grad_func_contain_y`: `<class 'bool'> = True`

`output_grad`: `<class 'bool'> = False`

`fixed_atom_tensor`: `Optional[torch.Tensor] = None`

`batch_indices`: `Union[NoneType, List[int], Tuple[int, ...], torch.Tensor] = None`

)


        Run the Optimization Algorithm.

        Parameters:
            func: the main function of instantiated torch.nn.Module class.
            X: Tensor[n_batch, n_atom, 3], the atom coordinates that input to func.
            grad_func: user-defined function that grad_func(X, ...) returns the func's gradient at X. if None, grad_func(X, ...) = th.autograd.grad(func(X, ...), X).
            func_args: optional, other input of func.
            func_kwargs: optional, other input of func.
            grad_func_args: optional, other input of grad_func.
            grad_func_kwargs: optional, other input of grad_func.
            is_grad_func_contain_y: bool, if True, grad_func contains output of func followed by X
                i.e., grad = grad_func(X, y, *grad_func_args, **grad_func_kwargs), else grad = grad_func(X, *grad_func_args, **grad_func_kwargs)
            output_grad: bool, whether output gradient of last step.
            fixed_atom_tensor: Optional[th.Tensor], the indices of X that fixed.
            batch_indices: Sequence | th.Tensor | np.ndarray | None, the split points for given X, Element_list & V_init, must be 1D integer array_like.
                the format of batch_indices is same as `split_size_or_sections` in torch.split:
                batch_indices = (n1, n2, ..., nN) will split X, Element_list & V_init into N parts, and ith parts has ni atoms. sum(n1, ..., nN) = X.shape[1]

        Return:
            min func: Tensor(n_batch, ), the minimum of func.
            argmin func: Tensor(X.shape), the X corresponds to min func.
            grad of argmin func: Tensor(X.shape), only output when `output_grad` == True. The gradient of X corresponding to minimum.
        

## FIRE

(

`E_threshold`: `<class 'float'> = 0.001`

`F_threshold`: `<class 'float'> = 0.05`

`maxiter`: `<class 'int'> = 100`

`steplength`: `<class 'float'> = 1.0`

`alpha`: `<class 'float'> = 0.1`

`alpha_fac`: `<class 'float'> = 0.99`

`fac_inc`: `<class 'float'> = 1.1`

`fac_dec`: `<class 'float'> = 0.5`

`N_min`: `<class 'int'> = 5`

`device`: `str | torch.device = cpu`

`verbose`: `<class 'int'> = 2`

`kwargs`: `<class 'inspect._empty'>`

)

None

## run

(

`func`: `Any | torch.nn.modules.module.Module`

`X`: `<class 'torch.Tensor'>`

`grad_func`: `Any | torch.nn.modules.module.Module = None`

`func_args`: `Sequence = ()`

`func_kwargs`: `Optional[Dict] = None`

`grad_func_args`: `Sequence = ()`

`grad_func_kwargs`: `Optional[Dict] = None`

`is_grad_func_contain_y`: `<class 'bool'> = True`

`output_grad`: `<class 'bool'> = False`

`fixed_atom_tensor`: `Optional[torch.Tensor] = None`

`batch_indices`: `Union[List, Tuple, torch.Tensor] = None`

`elements`: `Optional[Sequence[Sequence[str | int]]] = None`

)


        Run the Conjugate gradient

        Parameters:
            func: the main function of instantiated torch.nn.Module class.
            X: Tensor[n_batch, n_atom, 3], the atom coordinates that input to func.
            grad_func: user-defined function that grad_func(X, ...) return the func's gradient at X. if None, grad_func(X, ...) = th.autograd.grad(func(X, ...), X).
            func_args: optional, other input of func.
            func_kwargs: optional, other input of func.
            grad_func_args: optional, other input of grad_func.
            grad_func_kwargs: optional, other input of grad_func.
            is_grad_func_contain_y: bool, if True, grad_func contains output of func followed by X i.e., grad = grad_func(X, y, ...), else grad = grad_func(X, ...)
            output_grad: bool, whether output gradient of last step.
            fixed_atom_tensor: Optional[th.Tensor], the indices of X that fixed.
            batch_indices: Sequence | th.Tensor | np.ndarray | None, the split points for given X, Element_list & V_init, must be 1D integer array_like.
                the format of batch_indices is same as `split_size_or_sections` in torch.split:
                batch_indices = (n1, n2, ..., nN) will split X, Element_list & V_init into N parts, and ith parts has ni atoms. sum(n1, ..., nN) = X.shape[1]
            elements: Optional[Sequence[Sequence[str | int]]], the Element of each given atom in X.

        Return:
            min func: Tensor(n_batch, ), the minimum of func.
            argmin func: Tensor(X.shape), the X corresponds to min func.
            grad of argmin func: Tensor(X.shape), only output when `output_grad` == True. The gradient of X corresponding to minimum.
